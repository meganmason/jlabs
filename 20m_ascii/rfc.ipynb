{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_msk_20m.asc'\n",
    "# elv_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_dem_20m.asc'\n",
    "# asp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_asp_20m.asc'\n",
    "# slp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_slp_20m.asc'\n",
    "\n",
    "# dep_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/ascii/20170129_SUPERsnow_depth_20m.asc' #just one depth image...\n",
    "# years = list(range(2013, 2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #open\n",
    "# msk_ = np.loadtxt(msk_20m_path, skiprows=6)\n",
    "# elv_ = np.loadtxt(elv_20m_path, skiprows=6)\n",
    "# asp_ = np.loadtxt(asp_20m_path, skiprows=7) \n",
    "# slp_ = np.loadtxt(slp_20m_path, skiprows=7) \n",
    "# dep_ = np.loadtxt(dep_20m_path, skiprows=7) / 10                      #convert mm to cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #flatten\n",
    "# msk = msk_.flatten()\n",
    "# elv = elv_.flatten()\n",
    "# asp = asp_.flatten()\n",
    "# slp = slp_.flatten()\n",
    "# dep = dep_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filters: (1) Tuolumne Basin, (2) Snow Covered Areas\n",
    "# elv = elv[(msk==1) & (dep>0)]\n",
    "# asp = asp[(msk==1) & (dep>0)]\n",
    "# slp = slp[(msk==1) & (dep>0)] \n",
    "# dep = dep[(msk==1) & (dep>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #standardize snow depth values\n",
    "# sdv = (dep - dep.mean()) / dep.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataframe\n",
    "# d = {'elv': elv, 'asp': asp, 'slp': slp, 'sdv': sdv}                 #data to be put in df\n",
    "# df_ = pd.DataFrame(d)                                                #create df\n",
    "# df = df_.dropna()                                                    #remove all rows with any NaN's\n",
    "# df.drop(df[df['asp'] < 0].index, inplace=True)                       #remove all -9999 in aspect values (i.e. the lakes, water bodies)\n",
    "# df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().values.sum() #check for NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sdv.min(), sdv.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #bin labels\n",
    "# # bin_labels = {'0-50': 0, '51-75': 1, '76-100': 2, '101-125': 3, '126-150': 4, '151-200': 5, '200-250': 6, '250+': 7}\n",
    "\n",
    "# #method 1\n",
    "# #bin SDV into categorical bins\n",
    "# def bin_sd(x):\n",
    "#     if -2 < x <= -1.5:\n",
    "#         return 0\n",
    "#     elif -1.5 < x <= -1:\n",
    "#         return 1\n",
    "#     elif -1 < x <= -0.5:\n",
    "#         return 2\n",
    "#     elif -0.5 < x <= 0:\n",
    "#         return 3\n",
    "#     elif 0 < x <= 0.5:\n",
    "#         return 4\n",
    "#     elif 0.5 < x <= 1:\n",
    "#         return 5\n",
    "#     elif 1 < x <= 1.5:\n",
    "#         return 6\n",
    "#     elif 1.5 < x <= 2:\n",
    "#         return 7\n",
    "#     elif 2 < x <= 2.5:\n",
    "#         return 8\n",
    "#     elif 2.5 < x <= 3:\n",
    "#         return 9\n",
    "#     elif x > 3:\n",
    "#         return 10\n",
    "\n",
    "# df['sd_bin'] = df['sdv'].apply(bin_sd)\n",
    "# # df['bin_labels'] = bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #give label names\n",
    "# labels = ['0-50', '51-75', '76-100', '101-125', '126-150', '151-200', '200-250', '251+']\n",
    "# labels = ['-2 - -1', '51-75', '76-100', '101-125', '126-150', '151-200', '200-250', '251+'] #FINISH FILLING OUT!\n",
    "# # df['sd_labels'] = pd.cut(df['dep'], bins=[0, 50, 75, 100, 125, 150, 200, 250, float('Inf')], labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop SDV....\n",
    "# df = df.drop(['sdv'], axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float_format', lambda x: '%.2f' % x)           #float value to the hundredths place\n",
    "# df.describe()                                                         #summary stats, 50%=median, note Aspect and Slope mins=0, ~2.48 million values in remaining dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=df[['elv', 'asp', 'slp']] #features\n",
    "# y=df['sd_bin'] #values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# clf=RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=clf.predict(X_test)\n",
    "# from sklearn import metrics\n",
    "# print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #feature importance\n",
    "# feature_imp = pd.Series(clf.feature_importances_, index=df.columns[:3]).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a bar plot\n",
    "# sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# # Add labels to your graph\n",
    "# plt.xlabel('Feature Importance Score')\n",
    "# plt.ylabel('Features')\n",
    "# plt.title(\"Visualizing Important Features\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN ALL FILES\n",
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(f, msk, elv, asp, slp):\n",
    "    \n",
    "    file_type = f.split('.')[-1]\n",
    "    \n",
    "    if file_type == 'asc':\n",
    "        dep_ = np.loadtxt(f, skiprows=7) / 10                      #convert mm to cm\n",
    "        dep = dep_.flatten()\n",
    "\n",
    "        #filters: (1) Tuolumne Basin, (2) Snow Covered Areas\n",
    "        elv = elv[(msk==1) & (dep>0)]\n",
    "        asp = asp[(msk==1) & (dep>0)]\n",
    "        slp = slp[(msk==1) & (dep>0)] \n",
    "        dep = dep[(msk==1) & (dep>0)]\n",
    "\n",
    "        #standardize snow depth values\n",
    "        sdv = (dep - dep.mean()) / dep.std()\n",
    "\n",
    "        #dataframe\n",
    "        d = {'elv': elv, 'asp': asp, 'slp': slp, 'sdv': sdv}                 #data to be put in df\n",
    "        df_ = pd.DataFrame(d)                                                #create df\n",
    "        df = df_.dropna()                                                    #remove all rows with any NaN's\n",
    "        df.drop(df[df['asp'] < 0].index, inplace=True)                       #remove all -9999 in aspect values (i.e. the lakes, water bodies)\n",
    "\n",
    "    elif file_type == 'npy':\n",
    "        std_ = np.load(std_SDV_20m_path)\n",
    "        std = std_.flatten()\n",
    "    \n",
    "        #filters: (1) Tuolumne Basin, (2) Snow Covered Areas\n",
    "        elv = elv[msk==1]\n",
    "        asp = asp[msk==1]\n",
    "        slp = slp[msk==1] \n",
    "        std = std[msk==1]\n",
    "\n",
    "        #dataframe\n",
    "        d = {'elv': elv, 'asp': asp, 'slp': slp, 'sdv': std}                 #data to be put in df\n",
    "        df_ = pd.DataFrame(d)                                                #create df\n",
    "        df = df_.dropna()                                                    #remove all rows with any NaN's\n",
    "        df.drop(df[df['asp'] < 0].index, inplace=True)                       #remove all -9999 in aspect values (i.e. the lakes, water bodies)\n",
    "\n",
    "    \n",
    "    return df\n",
    " \n",
    "def bin_sd(x):\n",
    "    if 0 <= x <= 0.1:\n",
    "        return 0\n",
    "    elif 0.1 < x <= 0.2:\n",
    "        return 1\n",
    "    elif 0.2 < x <= 0.3:\n",
    "        return 2\n",
    "    elif 0.3 < x <= 0.4:\n",
    "        return 3\n",
    "    elif 0.4 < x <= 0.5:\n",
    "        return 4\n",
    "    elif 0.5 < x <= 0.6:\n",
    "        return 5\n",
    "    elif 0.6 < x <= 0.7:\n",
    "        return 6\n",
    "    elif 0.7 < x <= 0.8:\n",
    "        return 7\n",
    "    elif 0.8 < x <= 0.9:\n",
    "        return 8\n",
    "    elif 0.9 < x <= 1.0:\n",
    "        return 9\n",
    "    elif x > 1.0:\n",
    "        return 10\n",
    "    \n",
    "#     def bin_sd(x):\n",
    "#     if -2 < x <= -1.5:\n",
    "#         return 0\n",
    "#     elif -1.5 < x <= -1:\n",
    "#         return 1\n",
    "#     elif -1 < x <= -0.5:\n",
    "#         return 2\n",
    "#     elif -0.5 < x <= 0:\n",
    "#         return 3\n",
    "#     elif 0 < x <= 0.5:\n",
    "#         return 4\n",
    "#     elif 0.5 < x <= 1:\n",
    "#         return 5\n",
    "#     elif 1 < x <= 1.5:\n",
    "#         return 6\n",
    "#     elif 1.5 < x <= 2:\n",
    "#         return 7\n",
    "#     elif 2 < x <= 2.5:\n",
    "#         return 8\n",
    "#     elif 2.5 < x <= 3:\n",
    "#         return 9\n",
    "#     elif x > 3:\n",
    "#         return 10\n",
    "    \n",
    "def rfc(df):\n",
    "    \n",
    "    X=df[['elv', 'asp', 'slp']] #features\n",
    "    y=df['sd_bin'] #values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) #train/test split\n",
    "    clf=RandomForestClassifier(n_estimators=100)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred=clf.predict(X_test)\n",
    "\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred) * 100) #accuracy \n",
    "    \n",
    "    feature_imp = pd.Series(clf.feature_importances_, index=df.columns[:3]).sort_values(ascending=False) #feature importance\n",
    "\n",
    "    featImp_elv = round(feature_imp['elv'] * 100)\n",
    "    featImp_asp = round(feature_imp['asp'] * 100)\n",
    "    featImp_slp = round(feature_imp['slp'] * 100)\n",
    "    \n",
    "    return acc, featImp_elv, featImp_asp, featImp_slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run ascii files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import data\n",
    "# msk_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_msk_20m.asc'\n",
    "# elv_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_dem_20m.asc'\n",
    "# asp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_asp_20m.asc'\n",
    "# slp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_slp_20m.asc'\n",
    "# dep_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/ascii/*.asc' \n",
    "# years = list(range(2013, 2019))\n",
    "\n",
    "# #handle 'fixed' terrain layers\n",
    "\n",
    "# #open\n",
    "# msk_ = np.loadtxt(msk_20m_path, skiprows=6)\n",
    "# elv_ = np.loadtxt(elv_20m_path, skiprows=6)\n",
    "# asp_ = np.loadtxt(asp_20m_path, skiprows=7) \n",
    "# slp_ = np.loadtxt(slp_20m_path, skiprows=7)\n",
    "\n",
    "# #flatten\n",
    "# msk = msk_.flatten()\n",
    "# elv = elv_.flatten()\n",
    "# asp = asp_.flatten()\n",
    "# slp = slp_.flatten()\n",
    "\n",
    "# flist = glob.glob(dep_20m_path)\n",
    "# flist = [flist[i] for i in (0,7,18,30,42,49)] #just peak SDV\n",
    "\n",
    "# #-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# dates = []\n",
    "# accs = []\n",
    "# featImp_elvs = []\n",
    "# featImp_asps = []\n",
    "# featImp_slps = []\n",
    "\n",
    "\n",
    "# for i, f in enumerate(sorted(flist)):\n",
    "              \n",
    "                       \n",
    "#     print('processing file {}:'.format(i), f.split('/')[-1])\n",
    "    \n",
    "#     dt_str = f.split(\"/\")[-1] #splits on '/' and saves the last one\n",
    "#     dt_str = \"\".join([c for c in dt_str if c.isnumeric()]) #grabs numeric values for date info\n",
    "#     dates.append(dt_str[:-2]) #append to date list\n",
    "    \n",
    "#     df = prep_data(f, msk, elv, asp, slp)\n",
    "    \n",
    "#     df['sd_bin'] = df['sdv'].apply(bin_sd)\n",
    "    \n",
    "#     df = df.drop(['sdv'], axis=1)\n",
    "    \n",
    "#     acc, featImp_elv, featImp_asp, featImp_slp = rfc(df)\n",
    "    \n",
    "#     accs.append(acc)    \n",
    "#     featImp_elvs.append(featImp_elv)\n",
    "#     featImp_asps.append(featImp_asp)\n",
    "#     featImp_slps.append(featImp_slp)\n",
    "#     print('~~round complete~~')\n",
    "    \n",
    "# dates = pd.to_datetime(dates, format='%Y%m%d')\n",
    "\n",
    "# print('COMPLETE!')\n",
    "\n",
    "\n",
    "# #-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# dataframe = pd.DataFrame(list(zip(pd.to_datetime(dates), accs, featImp_elvs, featImp_asps, featImp_slps)),\n",
    "#                         columns = ['Date', 'RFC Accuracy (%)', 'Elevation Importance (%)', 'Aspect Importance (%)', 'Slope Importance (%)'])\n",
    "# dataframe\n",
    "# dataframe.to_csv('/Users/meganmason491/Documents/research/sierra/analysis/results/tmp/SDV_peak_featureImportance.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file 0: std_SDV_2015_woZeros.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/data_analysis/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~round complete~~\n",
      "COMPLETE!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RFC Accuracy (%)</th>\n",
       "      <th>Elevation Importance (%)</th>\n",
       "      <th>Aspect Importance (%)</th>\n",
       "      <th>Slope Importance (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RFC Accuracy (%)  Elevation Importance (%)  Aspect Importance (%)  \\\n",
       "0              36.0                      39.0                   31.0   \n",
       "\n",
       "   Slope Importance (%)  \n",
       "0                  30.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "msk_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_msk_20m.asc'\n",
    "elv_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_dem_20m.asc'\n",
    "asp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_asp_20m.asc'\n",
    "slp_20m_path = '/Users/meganmason491/Documents/research/sierra/data/20m_analysis/terrain/tuolumne_slp_20m.asc'\n",
    "# std_SDV_20m_path = '/Users/meganmason491/Documents/research/sierra/analysis/results/output/std_SDV_peak_woZeros.npy'\n",
    "# std_SDV_20m_path = '/Users/meganmason491/Documents/research/sierra/analysis/results/output/std_SDV_2016_woZeros.npy'\n",
    "std_SDV_20m_path = '/Users/meganmason491/Documents/research/sierra/analysis/results/output/std_SDV_2015_woZeros.npy'\n",
    "\n",
    "#handle 'fixed' terrain layers\n",
    "\n",
    "#open\n",
    "msk_ = np.loadtxt(msk_20m_path, skiprows=6)\n",
    "elv_ = np.loadtxt(elv_20m_path, skiprows=6)\n",
    "asp_ = np.loadtxt(asp_20m_path, skiprows=7) \n",
    "slp_ = np.loadtxt(slp_20m_path, skiprows=7)\n",
    "\n",
    "#flatten\n",
    "msk = msk_.flatten()\n",
    "elv = elv_.flatten()\n",
    "asp = asp_.flatten()\n",
    "slp = slp_.flatten()\n",
    "\n",
    "flist = glob.glob(std_SDV_20m_path)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "accs = []\n",
    "featImp_elvs = []\n",
    "featImp_asps = []\n",
    "featImp_slps = []\n",
    "\n",
    "for i, f in enumerate(sorted(flist)):\n",
    "              \n",
    "                       \n",
    "    print('processing file {}:'.format(i), f.split('/')[-1])\n",
    "    \n",
    "    df = prep_data(f, msk, elv, asp, slp)\n",
    "    \n",
    "    df['sd_bin'] = df['sdv'].apply(bin_sd)\n",
    "    \n",
    "    df = df.drop(['sdv'], axis=1)\n",
    "    \n",
    "    acc, featImp_elv, featImp_asp, featImp_slp = rfc(df)\n",
    "    \n",
    "    accs.append(acc)    \n",
    "    featImp_elvs.append(featImp_elv)\n",
    "    featImp_asps.append(featImp_asp)\n",
    "    featImp_slps.append(featImp_slp)\n",
    "    print('~~round complete~~')\n",
    "\n",
    "print('COMPLETE!')\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "dataframe = pd.DataFrame(list(zip(accs, featImp_elvs, featImp_asps, featImp_slps)),\n",
    "                        columns = ['RFC Accuracy (%)', 'Elevation Importance (%)', 'Aspect Importance (%)', 'Slope Importance (%)'])\n",
    "dataframe\n",
    "# dataframe.to_csv('/Users/meganmason491/Documents/research/sierra/analysis/results/tmp/std_SDV_of_peak_featureImportance.csv')\n",
    "# dataframe.to_csv('/Users/meganmason491/Documents/research/sierra/analysis/results/tmp/std_SDV_of_2016_featureImportance.csv')\n",
    "# dataframe.to_csv('/Users/meganmason491/Documents/research/sierra/analysis/results/tmp/std_SDV_of_2015_featureImportance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST example of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.DataFrame({\n",
    "#     'sepal length':iris.data[:,0],\n",
    "#     'sepal width':iris.data[:,1],\n",
    "#     'petal length':iris.data[:,2],\n",
    "#     'petal width':iris.data[:,3],\n",
    "#     'species':iris.target\n",
    "# })\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]\n",
    "# y=data['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #feature importance\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_imp= pd.Series(clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n",
    "# feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a bar plot\n",
    "# sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# # Add labels to your graph\n",
    "# plt.xlabel('Feature Importance Score')\n",
    "# plt.ylabel('Features')\n",
    "# plt.title(\"Visualizing Important Features\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

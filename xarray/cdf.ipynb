{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "# import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import earthpy as et\n",
    "import earthpy.spatial as es\n",
    "import earthpy.plot as ep\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "sns.set_context(\"talk\", font_scale=1.8) #[poster, paper, talk, notebook]\n",
    "\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.dates as mdates\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color'] #added from snow class\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 51, x: 1000, y: 1000)\n",
      "Coordinates:\n",
      "  * y        (y) float32 4221327.0 4221324.0 4221321.0 ... 4218333.0 4218330.0\n",
      "  * time     (time) datetime64[ns] 2013-04-03 2013-04-29 ... 2018-05-28\n",
      "    dem      (y, x) float32 dask.array<shape=(1000, 1000), chunksize=(1000, 1000)>\n",
      "  * x        (x) float32 279207.0 279210.0 279213.0 ... 282201.0 282204.0\n",
      "    veg      (y, x) float32 dask.array<shape=(1000, 1000), chunksize=(1000, 1000)>\n",
      "    mask     (y, x) uint8 dask.array<shape=(1000, 1000), chunksize=(1000, 1000)>\n",
      "Data variables:\n",
      "    snow     (time, y, x) int16 dask.array<shape=(51, 1000, 1000), chunksize=(1, 1000, 1000)>\n",
      "Attributes:\n",
      "    units:    cm\n"
     ]
    }
   ],
   "source": [
    "# fname = '~/Documents/projects/thesis/results/output/compiled_SUPERsnow.nc' #ARS\n",
    "# fname = '~/Documents/research/sierra/data/compiled_SUPERsnow.nc' #BSU\n",
    "# fname = '~/Documents/research/sierra/data/20m_analysis/compiled_SUPERsnow_20m.nc' #BSU\n",
    "fname = '/Users/megmason/research/sierra/data/compiled_SUPERsnow_3km_subregion.nc'\n",
    "#~~~~ ds full\n",
    "ds = xr.open_dataset(fname,  chunks={'time':1,'x':1000,'y':1000})\n",
    "ds.close()\n",
    "\n",
    "# #~~~~ ds peak\n",
    "# dpeak = ds.isel(time=[0,7,18,30,42,49]) \n",
    "# dpeak.close()\n",
    "# ds = dpeak\n",
    "# ds.close()\n",
    "#~~~convert to cm\n",
    "ds['snow'] = ds.snow / 10\n",
    "ds.attrs['units'] = 'cm'\n",
    "#grab 2015, 2016, 2017 nearest peak SWE\n",
    "# ds=ds.isel(time=slice(2,5))\n",
    "# ds.close()\n",
    "#northern slice of data\n",
    "# d = ds.isel(x=slice(8400,9400), y=slice(3000,4000))\n",
    "# d.close()\n",
    "\n",
    "#laptop\n",
    "d=ds\n",
    "d.close()\n",
    "#~~~convert to ints again\n",
    "d=d.astype(np.int16, copy=False)\n",
    "print(d)\n",
    "\n",
    "# d.to_netcdf('~/Documents/research/sierra/data/compiled_SUPERsnow_3km_subregion.nc')\n",
    "#dataset is d here! 3km by 3km area in northern Tuolumne (2 valleys west of slide canyon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terrain layers --- 3m analysis, use terrain.nc\n",
    "\n",
    "# path = '/home/meganmason/Documents/projects/thesis/data/processing_lidar/depths_3m/equal_extent/terrain/*.nc' #ARS\n",
    "t_file = '~/Documents/research/sierra/data/terrain.nc' #BSU\n",
    "\n",
    "terrain = xr.open_dataset(t_file)#,  chunks={'time':1,'x':1000,'y':1000})\n",
    "terrain.close()\n",
    "terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terr = terrain.isel(x=slice(8400,9400), y=slice(3000,4000))\n",
    "plt.imshow(terr.aspect)\n",
    "print(terr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_interval=np.arange(1500,4000,50) #50m interval\n",
    "\n",
    "#plot it\n",
    "which_date=30\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.contour(d.dem, levels=c_interval, colors='dimgray', linewidths=1)#, alpha=.8)\n",
    "plt.imshow(d.snow.isel(time=which_date),cmap='Blues', vmax=500)\n",
    "title=(d['time'].dt.strftime('%Y-%m-%d')[which_date])\n",
    "plt.title(str(title.values))\n",
    "plt.colorbar(label='Snow depth [cm]')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(terr.aspect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot it- aspect\n",
    "which_date=30\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# terrain=np.flip(terrain.aspect,0) #terrain needs to be flipped!\n",
    "# terrain=terrain.where(ds.mask==1)\n",
    "# terrain=terrain.to_dataset()\n",
    "# terr = terrain.isel(x=slice(8400,9400), y=slice(3000,4000))\n",
    "\n",
    "\n",
    "# color_count = 0\n",
    "# colors = ['Khaki', 'MediumSlateBlue', 'LightSeaGreen', 'Coral', 'Khaki']\n",
    "a=np.array([[0,45,135,225,315,360]])\n",
    "mycmap = mpl.colors.ListedColormap(['Khaki', 'MediumSlateBlue', 'LightSeaGreen', 'Coral', 'Khaki'])\n",
    "norm = mpl.colors.BoundaryNorm(a[0], len(a[0])-1)\n",
    "img = plt.imshow(a, cmap=mycmap, norm=norm)\n",
    "plt.show\n",
    "# c_map = plt.cm.get_cmap('Set2', 5)\n",
    "\n",
    "plt.contour(d.dem, levels=c_interval, colors='dimgray', linewidths=1)#, alpha=.8)\n",
    "plt.imshow(terr.aspect, cmap=mycmap, norm=norm)\n",
    "title=(d['time'].dt.strftime('%Y-%m-%d')[which_date])\n",
    "plt.title('Aspect')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#colorbar\n",
    "a=np.array([[0,45,135,225,315]])\n",
    "mycmap = mpl.colors.ListedColormap(['Khaki', 'MediumSlateBlue', 'LightSeaGreen', 'Coral'])\n",
    "m = np.zeros((1,360))\n",
    "for i in range(360):\n",
    "    m[0,i] = (i*5)/100.0\n",
    "\n",
    "plt.imshow(m, cmap=mycmap, aspect=50)\n",
    "plt.yticks(np.arange(0))\n",
    "plt.xticks(np.arange(45,360,90), ['north','east','south','west'], ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt to fully use Xarray.....(right track, needs dimention work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute stats from xarray dataset\n",
    "x_gt0 = d.snow.where(d.snow>0) #snow depths greater than zero\n",
    "mu_gt0 = x_gt0.mean(dim=('x', 'y'))\n",
    "sig_gt0 = x_gt0.std(dim=('x', 'y'))\n",
    "\n",
    "mu=mu_gt0 #51 means over x,y dimention\n",
    "sigma=sig_gt0 #51 stds over x,y dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to show you have values >0\n",
    "which_date=1\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imshow(x_gt0.isel(time=which_date), cmap='Blues', vmax=250)\n",
    "plt.colorbar()\n",
    "title=(d['time'].dt.strftime('%Y-%m-%d')[which_date])\n",
    "plt.title(str(title.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) set number of bins to use\n",
    "n_bins=100\n",
    "\n",
    "#2) compute histogram and bin edges for each lidar layer (dim=time)\n",
    "    ######~~NOTE - likely need to compute using a dimention, i.e. result should be multiple hist,bin returns\n",
    "hist, bins = np.histogram(x_gt0.values[~np.isnan(x_gt0.values)], bins=n_bins, density=True)\n",
    "\n",
    "#3) compute y\n",
    "#~~NOTE - only working because it grabs one value from mu and sigma instead of working through a dim\n",
    "y = norm.pdf(bins, mu.values[0], sigma.values[0]).cumsum()\n",
    "#get rid of last value of y (bins and y are same size)\n",
    "y /= y[-1]\n",
    "\n",
    "#4) plot it \n",
    "plt.plot(bins, y, linestyle='--', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hist is length:', len(hist), 'type:', type(hist))\n",
    "print('bins is length:', len(bins), 'type:', type(bins))\n",
    "print('mu is length:', len(mu), 'type:', type(mu))\n",
    "print('sigma is length:', len(sigma), 'type:', type(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try with Will\n",
    "problem: hist and bins computes over full dataset\n",
    "y uses bins, mu, and sigma to compute. \n",
    "    bins - all data being used\n",
    "    mu and sigma - one values per lidar flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) set number of bins to use\n",
    "n_bins=100\n",
    "\n",
    "#2) compute histogram and bin edges for each lidar layer (dim=time)\n",
    "#~~NOTE - likely need to compute using a dimention, i.e. result should be multiple hist,bin returns\n",
    "hist, bins = np.histogram(x_gt0.values[~np.isnan(x_gt0.values)], bins=n_bins, density=True)\n",
    "\n",
    "#3) compute y\n",
    "#~~NOTE - only working because it grabs one value from mu and sigma instead of working through a dim\n",
    "# y = norm.pdf(bins, mu.values[0], sigma.values[0]).cumsum()\n",
    "y = norm.pdf(bins, mu, sigma).cumsum()\n",
    "#get rid of last value of y (bins and y are same size)\n",
    "y /= y[-1]\n",
    "\n",
    "#4) plot it \n",
    "plt.plot(bins, y, linestyle='--', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt0.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy rescue (using code from 50m cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cdf(depth_array, n_bins):\n",
    "    \n",
    "    '''\n",
    "    Compute Cumulative Density Function (CDF). Solves the cumulative probability of a given x value.\n",
    "    Determines the probability that a random observation will be less/greater than a certain value.\n",
    "    \n",
    "    What is the probability (read y-axis) that you get a snow depth measurement in the Tuolumne Basin\n",
    "    that is ____ meters deep (x_axis)? \n",
    "    \n",
    "    at the 1, you say: 100% of the time you get a measurement that is (x-axis) meters or less\n",
    "    at the .5 you say: 50% of the time you get a measurment that is (x-axis) meters or less\n",
    "    \n",
    "        args:\n",
    "            depth_array: ndarray of snow depth values\n",
    "            n_bins: number of bins\n",
    "          \n",
    "            \n",
    "        returns:\n",
    "            bins: bin edges (left edges and final right side edge)\n",
    "            y: cumulative sum of probability density function (PDF). \n",
    "            \n",
    "    '''\n",
    "    \n",
    "    d = depth_array[depth_array > 0] #data are inside masked area and >0, scalar\n",
    "    \n",
    "    mu = np.nanmean(d)\n",
    "    sigma = np.nanstd(d)\n",
    "    \n",
    "    hist, bins = np.histogram(d, bins=n_bins, density=True)\n",
    "    \n",
    "    y = norm.pdf(bins, mu, sigma).cumsum()\n",
    "    y /= y[-1]\n",
    "    \n",
    "    return bins, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CLOSE!!!!!\n",
    "\n",
    "# # ~~~ Annual CDF plots\n",
    "# fs_titles = 34\n",
    "# fs_labels = 30\n",
    "# fs_axes = 30\n",
    "# fs_text = 26\n",
    "# fs_legend = 30\n",
    "\n",
    "# n_bins=100 #number of bins in histogram\n",
    "# years = list(range(2013,2019)) #which years\n",
    "# date_df = d.time.to_dataframe() #store dates in dataframe\n",
    "\n",
    "# fig,axs=plt.subplots(2,3,figsize = (30, 20)) #make one figure, with axs (plot1, plot2, etc.)\n",
    "# flat_ax = axs.flat #flatten the axes (subplots)\n",
    "\n",
    "# for ii, year in enumerate(years):\n",
    "#     print('~~~YEAR~~~', year)\n",
    "#     ax = flat_ax[ii] #which plot are you plotting\n",
    "    \n",
    "\n",
    "#     startTime = pd.to_datetime(\"10-01-{}\".format(year-1))\n",
    "#     endTime = pd.to_datetime(\"09-30-{}\".format(year))\n",
    "\n",
    "#     current_df = date_df.loc[startTime:endTime] #buy pickles!\n",
    "\n",
    "#     for i,row in current_df.iterrows():\n",
    "#         f = row['time']\n",
    "#         print(f)        \n",
    "#         for layer in d.snow.values:\n",
    "\n",
    "#             bins,y = compute_cdf(layer, n_bins) #layer is ndarray\n",
    "\n",
    "#             ax.plot(bins, y,  linestyle='-', linewidth=2, label=str(i.date())) #color=cycle[i],\n",
    "#             ax.set_xlim(0,750)\n",
    "#         ax.set_title('{}'.format(year), fontsize = fs_titles-3)\n",
    "#         ax.set_xlabel('Snow Depth (cm)', fontsize = fs_axes-2)\n",
    "#         ax.set_ylabel('Probability', fontsize = fs_axes-2)\n",
    "#         ax.legend()\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # # plt.savefig('/home/meganmason/Documents/projects/thesis/figures/working/3m_depth/annual_cdf_subplots.png', dpi=150)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Annual CDF plots\n",
    "fs_titles = 34\n",
    "fs_labels = 30\n",
    "fs_axes = 30\n",
    "fs_text = 26\n",
    "fs_legend = 30\n",
    "\n",
    "n_bins=100 #number of bins in histogram\n",
    "years = list(range(2013,2019)) #which years\n",
    "# date_df = d.time.to_dataframe() #store dates in dataframe\n",
    "\n",
    "fig,axs=plt.subplots(2,3,figsize = (30, 25)) #make one figure, with axs (plot1, plot2, etc.)\n",
    "flat_ax = axs.flat #flatten the axes (subplots)\n",
    "\n",
    "for ii, year in enumerate(years):\n",
    "    print('~~~YEAR~~~', year)\n",
    "    ax = flat_ax[ii] #which plot are you plotting\n",
    "    \n",
    "    startTime = pd.to_datetime(\"10-01-{}\".format(year-1))\n",
    "    endTime = pd.to_datetime(\"09-30-{}\".format(year))\n",
    "\n",
    "    d_sub=d.sel(time=slice(startTime,endTime))\n",
    "\n",
    "    for array_index, time in enumerate(d_sub['time'].dt.strftime('%Y-%m-%d').values):\n",
    "    #for layer in d_sub.snow.values:\n",
    "\n",
    "        # array_index is a number between 0 and the length of the times in the 'cube'\n",
    "        # time is the string that corresponds to array_index, grabbed from the xarray dataframe \n",
    "\n",
    "        layer = d_sub.snow.values[array_index,:,:] # this grabs the 'array_index' number of the 'cube' with everything in the x,y direction\n",
    "\n",
    "        bins,y = compute_cdf(layer, n_bins) #layer is ndarray\n",
    "        ax.plot(bins, y, linestyle='-', label=time) #color=cycle[i],linewidth=2,\n",
    "        ax.set_xlim(0,800)\n",
    "    ax.axhline(y=0.75, linestyle='--', color='k') #linewidth='3'\n",
    "    ax.axhline(y=0.5, linestyle='--', color='k') #linewidth='3'\n",
    "    ax.axhline(y=0.25, linestyle='--',color='k') #linewidth='3'\n",
    "    ax.set_title('{}'.format(year), fontsize = fs_titles)\n",
    "    ax.set_xlabel('Snow Depth (cm)')#, fontsize = fs_axes)\n",
    "    ax.set_ylabel('Cumulative frequency, %')#, fontsize = fs_axes)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figs/annual_cdf_subplots.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extra stuff...ie good code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp in d.groupby('time.year').groups:\n",
    "    print(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby('time.year').groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

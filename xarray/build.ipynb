{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PATHS- set for multiple computers, see bottom to SET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASO lidar - netCDFs & integers in [mm]\n",
    "ars_path = '/home/meganmason/Documents/projects/thesis/data/processing_lidar/depths_3m/equal_extent_data_downsize/nc/20*/*int.nc'\n",
    "# mac_path = ''\n",
    "# bsu_path = ''\n",
    "sonic_path = '/home/meganmason/Research/Sierra/data/mm/20*/*int.nc'\n",
    "geohack_path = '/srv/shared/deep_stac/data/snowdepth/netcdf/20*/*int.nc'\n",
    "\n",
    "#DEM & Mask for Tuolumne\n",
    "ars_topo_path = '/home/meganmason/Documents/projects/thesis/maps/map_layers/basin_ops/tuolumne/topo/basin_setup/topo.nc'\n",
    "# mac_topo_path =\n",
    "# bsu_topo_path =\n",
    "sonic_topo_path = '/home/meganmason/Research/Sierra/data/topo.nc'\n",
    "geohack_topo_path = '/srv/shared/deep_stac/data/topo.nc'\n",
    "\n",
    "\n",
    "# ~~ SET PATH HERE ~~ #\n",
    "path = ars_path\n",
    "topo_path = ars_topo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FILEPATH LIST AND DATE PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lidar files as input: 51\n",
      "dates, file type is: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n"
     ]
    }
   ],
   "source": [
    "#filepath list, lidar\n",
    "flist = sorted(glob.glob(path))\n",
    "print('# of lidar files as input:', len(flist))\n",
    "\n",
    "# Parse dates, store as pandas datetime type in list\n",
    "fparse = [d.split('/')[-1] for d in flist] #splits fname on '/' saves last\n",
    "fparse = [d.split('_')[0] for d in fparse] #splits fname on '_' saves first\n",
    "\n",
    "# flight dates, parsed from filename\n",
    "dates = pd.to_datetime(fparse) #pandas datetime variable, lidar dates\n",
    "print('dates, file type is:', type(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTION: run a smaller chunk, peak, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''first 2 files'''\n",
    "# flist = flist[:2] #first 6 are just 2013\n",
    "# dates = dates[:2]\n",
    "\n",
    "'''peak snow depth dates....(?)'''\n",
    "flist = list(flist[i] for i in (0,7,22,28,41,49))\n",
    "dates = list(dates[i] for i in (0,7,22,28,41,49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OPEN DATASET AND BUILD XARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 6, x: 17569, y: 17002)\n",
       "Coordinates:\n",
       "  * x        (x) float64 2.54e+05 2.54e+05 2.54e+05 ... 3.067e+05 3.067e+05\n",
       "  * y        (y) float64 4.179e+06 4.179e+06 4.179e+06 ... 4.23e+06 4.23e+06\n",
       "  * time     (time) datetime64[ns] 2013-04-03 2014-04-07 ... 2018-04-23\n",
       "Data variables:\n",
       "    snow     (time, y, x) int16 dask.array<shape=(6, 17002, 17569), chunksize=(1, 1000, 1000)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load xarray, via multiple files\n",
    "ds=xr.open_mfdataset(flist, concat_dim='time', combine='nested', chunks={'x':1000, 'y':1000}, parallel=True).rename({'Band1':'snow'})\n",
    "ds.close()\n",
    "\n",
    "ds = ds.drop('transverse_mercator') #remove\n",
    "ds = ds.astype(np.int16, copy=False) #cast as int16 (32,767)\n",
    "ds['time'] = (['time'], dates) #add 'time' coordinate, default populates with dimension called 'time'\n",
    "# ds['snow'].values = ds.snow[::-1] # flip along y-axis (maybe do that as function, call: preprocess= <func_name> in open_mfds())\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open topo.nc\n",
    "topo = xr.open_dataset(topo_path, drop_variables=['veg_tau','veg_k','veg_type','projection'], chunks={'x':1000, 'y':1000})\n",
    "topo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(ds.isel(time=1).snow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
